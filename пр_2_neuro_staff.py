# -*- coding: utf-8 -*-
"""ПР 2. neuro-staff

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uR8wCcseP-PJ-BW0giVpWwX3Pu2cO-f4

# Задача

**Создание команды нейро-сотрудников для онлайн-магазина**

**Цель:** Разработать и внедрить команду нейро-сотрудников, которые будут работать в онлайн-магазине, обеспечивая высококачественное обслуживание клиентов и эффективное управление продажами.


**Задачи нейро-сотрудников:**

*   Консультанты по продажам(call-center): Отвечать на вопросы клиентов, рассказывать о скидках и акциях, предлагать одежду и аксессуары, соответствующие их предпочтениям и интересам.
*   Менеджеры по набору персонала: Проводить набор сотрудников в магазин, рассматривать резюме, предлагать вакансии и оценивать кандидатов на соответствие требованиям магазина.
*   Менеджеры по продажам(Управляющий): Отслеживать выполнение плана по продажам, качество обратной связи с клиентом, принимать решения о продвижении или увольнении сотрудников, а также разрабатывать стратегии по увеличению продаж и улучшению качества обслуживания.


**Требования к нейро-сотрудникам:**

Высокий уровень интеллекта и способности к обучению

*   Хорошее понимание потребностей и поведения клиентов
*   Умение общаться с клиентами и сотрудниками на высоком уровне
*   Способность анализировать данные и принимать решения на основе результатов

**Преимущества:**

Повышение качества обслуживания клиентов
Увеличение продаж и доходов магазина
Сокращение времени и затрат на набор и обучение сотрудников
Улучшение качества обратной связи с клиентом и повышение лояльности
Результаты:

Создание команды нейро-сотрудников, способных обеспечить высококачественное обслуживание клиентов и эффективное управление продажами
Повышение продаж и доходов магазина
Улучшение качества обратной связи с клиентом и повышение лояльности

**Вывод:**
Создание команды нейро-сотрудников для онлайн-магазина - это инновационный подход к управлению продажами и обслуживанию клиентов. Благодаря высокому уровню интеллекта и способности к обучению, нейро-сотрудники смогут обеспечить высококачественное обслуживание клиентов, увеличить продажи и доходы магазина, а также улучшить качество обратной связи с клиентом.

#Интерфейс нейро-сотрудника Gradio
"""

!pip install openai gradio tiktoken langchain langchain-openai langchain-community chromadb

!pip install llama_index pyvis Ipython
!pip install peft
!pip install llama-index-readers-wikipedia wikipedia

import getpass # для работы с паролями
import os      # для работы с окружением и файловой системой

# Запрос ввода ключа от OpenAI
os.environ["OPENAI_API_KEY"] = getpass.getpass("Введите OpenAI API Key:")

models = [
              {
                "doc": "https://docs.google.com/document/d/1_z4s4Gr_yeGEf3zPPvbnPVoPQnzqTHGa1rcgilN5xFM/edit?usp=sharing",
                "prompt": '''Ты менеджер по подбору персонала в магазин одежды, перед тобой документ на должность продавца-консультанта.
                          Твоя задача задавать вопросы кандидату и придумывать вопросы.
                        Документ с информацией: ''',
                "name": "Специалист по набору кадров",
                "query": "Задавай вопросы"
              },
              {
                "doc": "https://docs.google.com/document/d/16B2dMioVG-6NZ64QOa0JSgYMkxeDC4xzKDgVrgA86tQ/edit?usp=sharing",
                "prompt": '''Ты специалист Call-Centra. Перед тобой документ, в котором находится информация которую спрашивают клиенты.
                        Твоя задача быть вежливым и отвечать на все вопросы по теме товаров и услуг, а так же говорить им какие у нас скидки или акции имеются.
                        Документ: ''',
                "name": "Специалист-Call Centra",
                "query": "Какие бренды у вас есть?"
              },
              {
                "doc": "https://docs.google.com/document/d/1_NZdqwnA3Rld3hDOejjEKQSdz6nzOe26mPhTFqszVEA/edit?usp=sharing",
                "prompt": '''Ты менеджер контроля качества и управляющий магазином, твоя задача анализировать диалоги менеджеров по продажам с клиентами и готовить отчеты.
                        Компания продает одежду.
                        Твоя задача делать отчеты по данному диалогу по запросам пользователя.
                        Составляй вопросы максимально точно по диалогу, не придумывай ничего от себя.
                        Текст диалога: ''',
                "name": "Управляющий(Оценка качества по диалогу)",
                "query": "Напиши отчет, какие были потребности названы клиентом"
              }
            ]

"""Составление документа сотрудников. Я использовал чат гпт, в котором писал где я буду использовать информацию, для кого пишется, а так же с какой целью нужно будет обращаться к человеку"""

# Блок библиотек фреймворка LangChain

# Работа с документами в langchain
from langchain.docstore.document import Document
# Эмбеддинги для OpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
# Доступ к векторной базе данных
from langchain.vectorstores import Chroma
# Разделение текста на куски или чанки (chunk)
from langchain.text_splitter import CharacterTextSplitter

# Отправка запросов
import requests

#Доступ к OpenAI
from openai import OpenAI

# Отприсовка интерфейса с помощью grad
import gradio as gr

# Библиотека подсчёта токенов
# Без запроcов к OpenAI, тем самым не тратим деньги на запросы
import tiktoken

# Для работы с регулярными выражениями
import re

# Объявляем класс нейро-сотрудника
class GPT():
    # Объявляем конструктор класса, для передачи имени модели и инициализации атрибутов класса
    def __init__(self, model="gpt-3.5-turbo"):
        self.log = ''               # атрибут для сбора логов (сообщений)
        self.model = model          # атрибут для хранения выбранной модели OpenAI
        self.search_index = None    # атрибут для хранения ссылки на базу знаний (если None, то модель не обучена)
        self.client = OpenAI(api_key=os.environ["OPENAI_API_KEY"]) # при инициализации запрашиваем ключ от OpenAI

    # Метод загрузки текстового документа в векторную базу знаний
    def load_search_indexes(self, url):
        # Извлекаем document ID гугл документа из URL с помощью регулярных выражений
        match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)

        # Если ID не найден - генерируем исключение
        if match_ is None:
            raise ValueError('Неверный Google Docs URL')

        # Первый элемент в результате поиска
        doc_id = match_.group(1)

        # Скачиваем гугл документ по его ID в текстовом формате
        response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')

        # При неудачных статусах запроса будет вызвано исключение
        response.raise_for_status()

        # Извлекаем данные как текст
        text = response.text

        # Вызываем метод векторизации текста и сохранения в векторную базу данных
        return self.create_embedding(text)

    # Подсчет числа токенов в строке по имени модели
    def num_tokens_from_string(self, string):
            """Возвращает число токенов в строке"""
            encoding = tiktoken.encoding_for_model(self.model)  # получаем кодировщик по имени модели
            num_tokens = len(encoding.encode(string))           # расчитываем строку с помощью кодировщика
            return num_tokens                                   # возвращаем число токенов

    # Метод разбора текста и его сохранение в векторную базу знаний
    def create_embedding(self, data):
        # Список документов, полученных из фрагментов текста
        source_chunks = []
        # Разделяем текст на строки по \n (перенос на новую строку) или длине фрагмента (chunk_size=1024) с помощью сплитера
        # chunk_overlap=0 - означает, что фрагменты не перекрываются друг с другом.
        # Если больше нуля, то захватываем дополнительное число символов от соседних чанков.
        splitter = CharacterTextSplitter(separator="\n", chunk_size=1024, chunk_overlap=0)

        # Применяем splitter (функцию расщепления) к данным и перебираем все получившиеся чанки (фрагменты)
        for chunk in splitter.split_text(data):
            # LangChain работает с документами, поэтому из текстовых чанков мы создаем фрагменты документов
            source_chunks.append(Document(page_content=chunk, metadata={}))

        # Подсчет числа токенов в документах без запроса к OpenAI (экономим денежные средства)
        count_token = self.num_tokens_from_string(' '.join([x.page_content for x in source_chunks]))
        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации
        self.log += f'Количество токенов в документе : {count_token}\n'

        # Создание индексов документа. Применяем к нашему списку документов эмбеддингов OpenAi и в таком виде загружаем в базу ChromaDB
        self.search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(), )
        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации
        self.log += f'Данные из документа загружены в в векторную базу данных\n'

        # Возвращаем ссылку на базу данных
        return self.search_index

    # Демонстрация более аккуратного расчета числа токенов в зависимости от модели
    def num_tokens_from_messages(self, messages, model):
        """Возвращает число токенов из списка сообщений"""
        try:
            encoding = tiktoken.encoding_for_model(model) # получаем кодировщик по имени модели
        except KeyError:
            print("Предупреждение: модель не создана. Используйте cl100k_base кодировку.")
            encoding = tiktoken.get_encoding("cl100k_base") # если по имени не нашли, то используем базовый для моделей OpenAI
        # Выбор модели
        if model in {
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-16k-0613",
            "gpt-4-0314",
            "gpt-4-32k-0314",
            "gpt-4-0613",
            "gpt-4-32k-0613",
            "gpt-4o",
            "gpt-4o-2024-05-13"
            }:
            tokens_per_message = 3 # дополнительное число токенов на сообщение
            tokens_per_name = 1    # токенов на имя
        elif model == "gpt-3.5-turbo-0301":
            tokens_per_message = 4  # каждое сообщение содержит <im_start>{role/name}\n{content}<im_end>\n
            tokens_per_name = -1  # если есть имя, то роль не указывается
        elif "gpt-3.5-turbo" in model:
            self.log += f'Внимание! gpt-3.5-turbo может обновиться в любой момент. Используйте gpt-3.5-turbo-0613. \n'
            return self.num_tokens_from_messages(messages, model="gpt-3.5-turbo-0613")
        elif "gpt-4" in model:
            self.log += f'Внимание! gpt-4 может обновиться в любой момент. Используйте gpt-4-0613. \n'
            return self.num_tokens_from_messages(messages, model="gpt-4-0613")
        else: # исключение, если модель не поддерживается
            raise NotImplementedError(
                f"""num_tokens_from_messages() не реализован для модели {model}."""
            )

        # Запускаем подсчет токенов
        num_tokens = 0                        # счетчик токенов
        for message in messages:              # цикл по всем сообщениям
            num_tokens += tokens_per_message  # прибовляем число токенов на каждое сообщение
            for key, value in message.items():
                num_tokens += len(encoding.encode(value)) # считаем токены в сообщении с помощью кодировщика
                if key == "name":                     # если встретили имя
                    num_tokens += tokens_per_name     # то добавили число токенов на
        num_tokens += 3                               # каждый ответ оборачивается в <|start|>assistant<|message|>
        return num_tokens                             # возвращаем число токенов


    # Метод запроса к языковой модели
    def answer_index(self, system, topic, temp = 1):
        # Проверяем обучена ли наша модель
        if not self.search_index:
            self.log += 'Модель необходимо обучить! \n'
            return ''

        # Выборка документов по схожести с запросом из векторной базы данных, topic- строка запроса, k - число извлекаемых фрагментов
        docs = self.search_index.similarity_search(topic, k=5)
        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации
        self.log += 'Выбираем документы по степени схожести с вопросом из векторной базы данных: \n '
        # Очищаем запрос от двойных пустых строк. Каждый фрагмент подписываем: Отрывок документа № и дальше порядковый номер
        message_content = re.sub(r'\n{2}', ' ', '\n '.join([f'Отрывок документа №{i+1}:\n' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))
        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации
        self.log += f'{message_content} \n'

        # В системную роль помещаем найденные фрагменты и промпт, в пользовательскую - вопрос от пользователя
        messages = [
            {"role": "system", "content": system + f"{message_content}"},
            {"role": "user", "content": topic}
        ]

        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации
        self.log += f"\n\nТокенов использовано на вопрос по версии TikToken: {self.num_tokens_from_messages(messages, self.model)}\n"


        # Запрос к языковой моделе
        completion = self.client.chat.completions.create(
            model=self.model,   # используемая модель
            messages=messages,  # список форматированных сообщений с ролями
            temperature=temp    # точность ответов модели
        )


        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации
        self.log += '\nСтатистика по токенам от языковой модели:\n'
        self.log += f'Токенов использовано всего (вопрос): {completion.usage.prompt_tokens} \n'       # Число токенов на вопрос по расчетам LLM
        self.log += f'Токенов использовано всего (вопрос-ответ): {completion.usage.total_tokens} \n'  # Число токенов на вопрос и ответ по расчетам LLM

        return completion.choices[0].message.content # возвращаем результат предсказания

# Объявляем экземпляр класса GPT (созданный ранее) и передаем ему в конструктор модель LLM, с которой будем работать
gpt = GPT("gpt-3.5-turbo")

# Gradio позволяет объединять элементы в блоки
blocks = gr.Blocks()

# Работаем с блоком
with blocks as demo:
    # Объявляем элемент выбор из списка (с подписью Данные), список выбирает из поля name нашей переменной models
    subject = gr.Dropdown([(elem["name"], index) for index, elem in enumerate(models)], label="Данные")
    # Здесь отобразиться выбранное имя name из списка
    name = gr.Label(show_label=False)
    # Промпт для запроса к LLM (по умолчанию поле prompt из models)
    prompt = gr.Textbox(label="Промт", interactive=True)
    # Ссылка на файл обучения (по умолчанию поле doc из models)
    link = gr.HTML()
    # Поле пользовательского запроса к LLM (по умолчанию поле query из models)
    query = gr.Textbox(label="Запрос к LLM", interactive=True)


    # Функция на выбор нейро-сотрудника в models
    # Ей передается параметр subject - выбранное значение в поле списка
    # А возвращаемые значения извлекаются из models
    def onchange(dropdown):
      return [
          models[dropdown]['name'],                               # имя возвращается без изменения
          re.sub('\t+|\s\s+', ' ', models[dropdown]['prompt']),   # в промте удаляются двойные пробелы \s\s+ и табуляция \t+
          models[dropdown]['query'],                              # запрос возвращается без изменения
          f"<a target='_blank' href = '{models[dropdown]['doc']}'>Документ для обучения</a>" # ссылка на документ оборачивается в html тег <a>  (https://htmlbook.ru/html/a)
          ]

    # При изменении значения в поле списка subject, вызывается функция onchange
    # Ей передается параметр subject - выбранное значение в поле списка
    # А возвращаемые значения устанавливаются в элементы name, prompt, query и link
    subject.change(onchange, inputs = [subject], outputs = [name, prompt, query, link])

    # Строку в gradio можно разделить на столбцы (каждая кнопка в своем столбце)
    with gr.Row():
        train_btn = gr.Button("Обучить модель")       # кнопка запуска обучения
        request_btn = gr.Button("Запрос к модели")    # кнопка отправки запроса к LLM

    # функция обучения
    def train(dropdown):
        # парсим документ и сохраняем его в базу данных
        gpt.load_search_indexes(models[dropdown]['doc'])
        return gpt.log

     #Фильтр для нежелательных слов
    def filter_query(query):
        # Добавьте сюда правила фильтрации, используя регулярные выражения или другие методы
        forbidden_words = ['Идеальный кандидат', 'Критерии оценки:']
        for word in forbidden_words:
            if word in query:
                return ''
        return query

    # Вызываем метод запроса к языковой модели из класса GPT
    def predict(p, q):
        # Фильтруем запрос перед вызовом метода answer_index
        q = filter_query(q)
        if not q:
            return ['Доступ запрещен', gpt.log]
        result = gpt.answer_index(
            p,
            q
        )
        # возвращает список из ответа от LLM и log от класса GPT
        return [result, gpt.log]

    # Выводим поля response с ответом от LLM и log (вывод сообщений работы класса GPT) на 2 колонки
    with gr.Row():
        response = gr.Textbox(label="Ответ LLM") # Текстовое поле с ответом от LLM
        log = gr.Textbox(label="Логирование")    # Текстовое поле с выводом сообщений от GPT


    # При нажатии на кнопку train_btn запускается функция обучения train_btn с параметром subject
    # Результат выполнения функции сохраняем в текстовое поле log - лог выполнения
    train_btn.click(train, [subject], log)

    # При нажатии на кнопку request_btn запускается функция отправки запроса к LLM request_btn с параметром prompt, query
    # Результат выполнения функции сохраняем в текстовые поля  response - ответ модели, log - лог выполнения
    request_btn.click(predict, [prompt, query], [response, log])

# Запуск приложения
demo.launch()

"""#Трассировка"""

!pip install arize-phoenix

import nest_asyncio
import phoenix as px

from phoenix.evals import (
    HallucinationEvaluator,
    OpenAIModel,
    QAEvaluator,
    RelevanceEvaluator,
    run_evals,
)
from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents
from phoenix.trace import DocumentEvaluations, SpanEvaluations

nest_asyncio.apply()  # необходим для параллельных вычислений в среде ноутбуков

import phoenix as px

session = px.launch_app()

"""К сожалению трассировку выполнить не получается из за особенностей платформы

#Проверка RAG и возможные галлюцинации
"""

!pip install openai llama_index "arize-phoenix[evals,llama-index]" gcsfs nest-asyncio "openinference-instrumentation-llama-index>=2.0.0"

from llama_index.core import SimpleDirectoryReader
from llama_index.core import KnowledgeGraphIndex
from llama_index.core import Settings
from llama_index.core.graph_stores import SimpleGraphStore
from llama_index.core import StorageContext
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch

"""В качестве данных мы будем использовать Pdf.

Особое внимание я хочу уделить нейро-сотруднику отвечающему за набор персонала,ответственному за формирование команды, которая будет работать над проектом. Это ключевая фигура, которая будет отвечать за поиск, отбор и найм кандидатов, соответствующих требованиям проекта.

В целом, нейро-сотрудник, отвечающий за набор персонала, играет ключевую роль в формировании команды, которая будет работать над проектом, и его успех напрямую зависит от его способности найти и привлечь лучших кандидатов.
"""

#Скачиваем документ
import os
import gdown
os.makedirs('./data', exist_ok=True)

file_id = "1dJ6gtY1Je64HZYCb9U9JE0AQ5zdQCbVc"
output_file = "./data/call.pdf"

gdown.download(f"https://drive.google.com/uc?id={file_id}", output_file)

# Загружаем файлы PDF
documents = SimpleDirectoryReader('./data').load_data()

import openai
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

from llama_index.core import (
    VectorStoreIndex,
    GPTVectorStoreIndex,
    SimpleDirectoryReader,
    KeywordTableIndex,
    StorageContext,
    load_index_from_storage,
    ServiceContext,
    Settings,
)

# Установим модель по умолчанию
Settings.llm = OpenAI(temperature=0, model='gpt-3.5-turbo')

index = GPTVectorStoreIndex.from_documents(
	documents
)
# Подготавливаем движок к индексу и задаем ему вопрос
query_engine = index.as_query_engine()

response = query_engine.query("Какое время работы?")
print(response)

"""Модель отлично себя показывает, отвечает на все вопросы которые ей задают"""

# Изменим модель

Settings.llm = OpenAI(temperature=0, model='gpt-4o')

index = GPTVectorStoreIndex.from_documents(
	documents
)
# Подготавливаем движок к индексу и задем ему вопрос
query_engine = index.as_query_engine()
response = query_engine.query("Какое время работы?")
print(response)

"""gpt-3.5-turbo отвечает на вопросы более подробно

Галлюцинации возможны в:

1.  "answer_index", поскольку этот метод отвечает за генерацию ответов пользователю, если языковая модель не правильно обучится на данных, то должного ответа не получить.
2.  "load_search_indexes", загружает документ в поисковой индекс и если не правильно настроен, то данные могут быть не точными.

#Создание графов знаний
"""

!pip install llama-index-readers-file

from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import (
    DocxReader,
    HWPReader,
    PDFReader,
    EpubReader,
    FlatReader,
    HTMLTagReader,
    ImageCaptionReader,
    ImageReader,
    ImageVisionLLMReader,
    IPYNBReader,
    MarkdownReader,
    MboxReader,
    PptxReader,
    PandasCSVReader,
    VideoAudioReader,
    UnstructuredReader,
    PyMuPDFReader,
    ImageTabularChartReader,
    XMLReader,
    PagedCSVReader,
    CSVReader,
    RTFReader,
)

# Скачиваем документы
import os
import gdown

os.makedirs('./data', exist_ok=True)

file_ids = ["1dJ6gtY1Je64HZYCb9U9JE0AQ5zdQCbVc","1PH7txKwBFhbv-zwJkfrJBNUBUOWadn0c", "11jbjBk7Nw1aT3Ypx3AULcN1IRzLYt3YK"]
output_files = ["./data/nabor.pdf","./data/upravl.pdf","./data/call.pdf"]

for file_id, output_file in zip(file_ids, output_files):
    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_file)

# Создаем простое графовое хранилище
graph_store = SimpleGraphStore()

# Устанавливаем информацию о хранилище в StorageContext
storage_context = StorageContext.from_defaults(graph_store=graph_store)

from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import PDFReader

# Создаем экземпляр PDFReader
parser = PDFReader()

# Определяем файловый экстрактор для файлов с расширением .pdf
file_extractor = {".pdf": parser}

# Создаем экземпляр SimpleDirectoryReader, указывая директорию и файловый экстрактор
documents = SimpleDirectoryReader(
    "./data",  # директория, из которой загружаются файлы
    file_extractor=file_extractor
).load_data()

# Запускаем генерацию индексов из документа с помощью KnowlegeGraphIndex
indexKG = KnowledgeGraphIndex.from_documents( documents=documents,               # данные для построения графов
                                           max_triplets_per_chunk=3,        # сколько обработывать триплетов связей для каждого блока данных
                                           show_progress=True,              # показывать процесс выполнения
                                           include_embeddings=True,         # включение векторных вложений в индекс для расширенной аналитики
                                           storage_context=storage_context) # куда сохранять результаты

from pyvis.network import Network
from IPython.display import display
import IPython

g = indexKG.get_networkx_graph(500)
net = Network(notebook=True,cdn_resources="in_line", directed=True)
net.from_nx(g)
net.show("graph.html")
net.save_graph("Knowledge_graph.html")

IPython.display.HTML(filename="/content/Knowledge_graph.html")

"""# **Заключение**

В данной работе я сделал базовых нейро-сотрудников- это искусственные интеллекты, которые могут выполнять различные задачи, обычно требующие человеческого интеллекта, такие как понимание естественного языка, анализданных, генерация текстов и изображений, и т.д. Они могут помочь в автоматизации многих процессов, таких как кадровый документооборот, взаимодействие с сотрудниками, обучение и развитие сотрудников, прогнозирование выгорания и увольнения сотрудников, внутренние коммуникации и маркетинг и продажи.

Нейро-сотрудники могут помочь в управлении персоналом, например, в кадровом документообороте, взаимодействии с сотрудниками, обучении и развитии сотрудников, прогнозировании выгорания и увольнения сотрудников. Они могут помочь в автоматизации многих процессов, таких как просмотр и анализ резюме, выбор кандидата, написание различных сопроводительных писем и ответов на резюме.

В маркетинге и продажах нейро-сотрудники могут помочь в генерации заголовков, текстов рекламных объявлений, статей, постов, презентаций и продающих видео.

В целом, нейро-сотрудники могут помочь в автоматизации многих процессов, требующих человеческого интеллекта, и могут помочь в увеличении эффективности и производительности компаний.

Были выполнены следующие работы:
1.    Создание нейро-сотрудников, их "внутреннего мира", и каким "инструкциям" они должны следовать.
2.   Базу знаний, структуру и данные.
3.   Проведен анализ на возможные галлюцинации.
4.   Несколько приемов по улучшению RAG системы.
5.   Безопасность сотрудников и исключение возможностей сказать "лишнего" пользователю, введением простой фильтрации.
6.   Произвел анализ работоспособности на наборе данных с языковыми моделями.
7.   Построил графы знаний для представления знаний, которые используются для организации и визуализации сложных данных.
"""